{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPv7Xx/X4331r2hzyN8TTj+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pusse-01/Question-answering-model-with-text-comparision/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-3yKyBBejBe"
      },
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cl_vyGYUV2F"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oldZIjeU_5Qu"
      },
      "source": [
        "pip install PyPDF2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg3T8hMWdunO"
      },
      "source": [
        "from googletrans import Translator\n",
        "import PyPDF2 \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import math\n",
        "import string\n",
        "import sys\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utNsgC3kefpj"
      },
      "source": [
        "translator = Translator()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Ort5fGCCHt"
      },
      "source": [
        "def pdfExtractor(file):\n",
        "    # creating a pdf file object \n",
        "    pdfFileObj = open(file, 'rb') \n",
        "    \n",
        "    # creating a pdf reader object \n",
        "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
        "        \n",
        "    # printing number of pages in pdf file \n",
        "    print(pdfReader.numPages) \n",
        "        \n",
        "    # creating a page object \n",
        "    pageObj = pdfReader.getPage(1) \n",
        "        \n",
        "    # extracting text from page \n",
        "    pageObj.extractText()\n",
        "        \n",
        "    # closing the pdf file object \n",
        "    pdfFileObj.close()\n",
        "    return pageObj.extractText()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwqCBlb9CErq"
      },
      "source": [
        "def translateE2S(text): \n",
        "    result = translator.translate(text,dest='si')\n",
        "    #print(result.text)\n",
        "    return result.text\n",
        "\n",
        "def translateS2E(text):\n",
        "    result = translator.translate(text,dest='en')\n",
        "    #print(result.text)\n",
        "    return result.text"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moxSqwXJWebf"
      },
      "source": [
        "predicted_answers = []\n",
        "def question_answer(question, text):\n",
        "    \n",
        "    input_ids = tokenizer.encode(question, text)\n",
        "    \n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    \n",
        "    sep_idx = input_ids.index(tokenizer.sep_token_id)    \n",
        "    num_seg_a = sep_idx+1    \n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    \n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b    \n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
        "\n",
        "    answer_start = torch.argmax(output.start_logits)\n",
        "    answer_end = torch.argmax(output.end_logits)    \n",
        "    if answer_end >= answer_start:\n",
        "        answer = tokens[answer_start]\n",
        "        for i in range(answer_start+1, answer_end+1):\n",
        "            if tokens[i][0:2] == \"##\":\n",
        "                answer += tokens[i][2:]\n",
        "            else:\n",
        "                answer += \" \" + tokens[i]\n",
        "                \n",
        "    if answer.startswith(\"[CLS]\"):\n",
        "        answer = \"Unable to find the answer to your question.\"\n",
        "    \n",
        "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))\n",
        "    sin = translateE2S(answer.capitalize())\n",
        "    print(sin)\n",
        "    predicted_answers.append(answer.capitalize())"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MCNlMZAErd9"
      },
      "source": [
        "# splitting the text lines into words\n",
        "# translation table is a global variable\n",
        "# mapping upper case to lower case and\n",
        "# punctuation to spaces\n",
        "translation_table = str.maketrans(string.punctuation+string.ascii_uppercase,\n",
        "                                     \" \"*len(string.punctuation)+string.ascii_lowercase)\n",
        "       \n",
        "# returns a list of the words\n",
        "# in the file\n",
        "def get_words_from_line_list(text): \n",
        "      \n",
        "    text = text.translate(translation_table)\n",
        "    word_list = text.split()\n",
        "      \n",
        "    return word_list\n",
        "  \n",
        "  \n",
        "# counts frequency of each word\n",
        "# returns a dictionary which maps\n",
        "# the words to  their frequency.\n",
        "def count_frequency(word_list): \n",
        "      \n",
        "    D = {}\n",
        "      \n",
        "    for new_word in word_list:\n",
        "          \n",
        "        if new_word in D:\n",
        "            D[new_word] = D[new_word] + 1\n",
        "              \n",
        "        else:\n",
        "            D[new_word] = 1\n",
        "              \n",
        "    return D\n",
        "  \n",
        "# returns dictionary of (word, frequency)\n",
        "# pairs from the previous dictionary.\n",
        "def word_frequencies_for_file(text): \n",
        "      \n",
        "    word_list = get_words_from_line_list(text)\n",
        "    freq_mapping = count_frequency(word_list)\n",
        "  \n",
        "    #print(len(text), \"characters, \", )\n",
        "    #print(len(word_list), \"words, \", )\n",
        "    #print(len(freq_mapping), \"distinct words\")\n",
        "  \n",
        "    return freq_mapping\n",
        "  \n",
        "  \n",
        "# returns the dot product of two documents\n",
        "def dotProduct(D1, D2): \n",
        "    Sum = 0.0\n",
        "      \n",
        "    for key in D1:\n",
        "          \n",
        "        if key in D2:\n",
        "            Sum += (D1[key] * D2[key])\n",
        "              \n",
        "    return Sum\n",
        "  \n",
        "# returns the angle in radians \n",
        "# between document vectors\n",
        "def vector_angle(D1, D2): \n",
        "    numerator = dotProduct(D1, D2)\n",
        "    denominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2))\n",
        "      \n",
        "    return math.acos(numerator / denominator)\n",
        "  \n",
        "  \n",
        "def documentSimilarity(text1, text2):\n",
        "      \n",
        "   # filename_1 = sys.argv[1]\n",
        "   # filename_2 = sys.argv[2]\n",
        "    sorted_word_list_1 = word_frequencies_for_file(text1)\n",
        "    sorted_word_list_2 = word_frequencies_for_file(text2)\n",
        "    distance = vector_angle(sorted_word_list_1, sorted_word_list_2)\n",
        "      \n",
        "    print(\"The distance between the documents is: % 0.6f (radians)\"% distance)\n",
        "    "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWcSITiqFvA9"
      },
      "source": [
        "# reading the text file\n",
        "# This functio will return a \n",
        "# list of the lines of text \n",
        "# in the file.\n",
        "def read_file(filename): \n",
        "      \n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            data = f.read()\n",
        "        return data\n",
        "      \n",
        "    except IOError:\n",
        "        print(\"Error opening or reading input file: \", filename)\n",
        "        sys.exit()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdfd_REGCMSs",
        "outputId": "e5b327b8-6ee2-43df-8f53-3a058361b01f"
      },
      "source": [
        "file = input(\"Enter your file path : \")\n",
        "\n",
        "#text = pdfExtractor(file)   \n",
        " \n",
        "text = input(\"\\nPlease enter your text: \\n\")\n",
        "question = input(\"\\nPlease enter your question: \\n\")\n",
        "en_text = translateS2E(text)\n",
        "while True:\n",
        "    question_answer(question, en_text)\n",
        "    \n",
        "    flag = True\n",
        "    flag_N = False\n",
        "    \n",
        "    while flag:\n",
        "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
        "        if response[0] == \"Y\":\n",
        "            question = input(\"\\nPlease enter your question: \\n\")\n",
        "            question = translateS2E(question)\n",
        "            flag = False\n",
        "        elif response[0] == \"N\":\n",
        "            print(\"\\nEnd of the program!\")\n",
        "            flag = False\n",
        "            flag_N = True\n",
        "            \n",
        "    if flag_N == True:\n",
        "        break"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your file path : j\n",
            "\n",
            "Please enter your text: \n",
            "කොරෝනා වෛරසය පාලනය සඳහා ෆයිසර් එන්නත භාවිතාවේදී අවශ්‍ය හානිකර තත්ත්වයන් ඇති වීමේ අවදානමක් ඇතැයි ශ්‍රී ලංකා පොදුජන පෙරමුණේ පාර්ලිමේන්තු මන්ත්‍රී මහාචාර්ය තිස්ස විතාරණ මහතා සඳහන් කරයි.  එම එන්නතේ ක‍්‍රියාකාරීත්වය පිළිබඳ විද්‍යානුකූල සැකයක් ඇතිව තිබෙන බවද ඔහු පෙන්වා දෙයි.  එනිසා එම එන්නත මෙරටට ගෙන්වීම හැකිතාක් සීමා කර විකල්ප එන්නත් ගෙන්වීමට කටයුතු කරන්නැයි ද ඔහු ඉල්ලා සිටී.  මිනිස් ශරීරයට සෘජුව ජාන ඇතුළු කිරීම සම්බන්ධයෙන් බැලූ විට කිසිවෙකුට අනාවකි කළ නොහැකි බවද පැවසූ ඔහු අනවශ්‍ය හානිකර දේ සිදුවීමට ඉඩ ඇතැයි ද පැවසීය.\n",
            "\n",
            "Please enter your question: \n",
            "What did Thissa say?\n",
            "\n",
            "Predicted answer:\n",
            "The korona virus is at risk of improving the necessary harmful conditions in pbiser vaccine . he pointed out that there is a scientific suspicion of action\n",
            "කොරෝනා වෛරසය PBISER එන්නතෙහි අත්යවශ්ය හානිකර තත්වයන් වැඩිදියුණු කිරීමේ අවදානමට ලක්ව ඇත.ඔහු පෙන්වා දුන්නේ ක්රියාව පිළිබඳ විද්යාත්මක සැකයක් ඇති බවයි\n",
            "\n",
            "Do you want to ask another question based on this text (Y/N)? N\n",
            "\n",
            "End of the program!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb80B9a0JAfd",
        "outputId": "2a40a6ac-b9f5-4d14-ccfd-aded9e321aa5"
      },
      "source": [
        "for index in range(len(predicted_answers)):\n",
        "  answer_given = input(\"Child's answer\")\n",
        "  answer_given = translateS2E(answer_given)\n",
        "  documentSimilarity(predicted_answers[index], answer_given)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Child's answerකොරෝනා වෛරසය පාලනය සඳහා ෆයිසර් එන්නත භාවිතාවේදී අවශ්‍ය හානිකර තත්ත්වයන් ඇති වීමේ අවදානමක් ඇතැයි \n",
            "The distance between the documents is:  0.755332 (radians)\n"
          ]
        }
      ]
    }
  ]
}